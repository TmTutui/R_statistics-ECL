---
title: "BE1- Régression linéaire"
author: "Tulio NAVARRO TUTUI, Filipe PENNA CERAVOLO SOARES"
course: "Statistique appliquée aux sciences de l'ingénieur"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document: default
---

<!-- 
    For reference:
    - http://rmarkdown.rstudio.com>
    - https://lms.fun-mooc.fr/c4x/UPSUD/42001S02/asset/RMarkdown.html
 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6, fig.height = 6)
```

# Exercice 01 - Prix de mise en vente des appartements à Grenoble

Dans ce éxercice, on s'interesse à étudier la rélation entre prix et surface des immeubles à Grenoble. 

```{r}
prix_table = read.table(file = "immo.txt", header = TRUE)
head(prix_table)

plot(prix_table$m2, prix_table$prix)
```

## 1. Proposer un premier modèle de régression

On commence par essayer de modéliser le problème avec une régression lineaire, avec l'estimateur de moindres carrés. 
<!-- On procede avec un test de non régression, en utilisant l'hypothèse de non régression: H0 : β1 = β2 = .. = βp = 0.   -->

```{r}
    model1 = lm(prix~., data = prix_table)
    summary(model1)
```

### 1.a La droite de régression 

```{r}
    plot(prix~m2, data = prix_table)
    abline(model1$coefficients, col = 'red')    
```

### 1.b Quel est le pourcentage de variance expliquée par cette régression ?

```{r}
    sse_1 <- sum((fitted(model1) - prix_table$prix)^2)

    ssr_1 <- sum((fitted(model1) - mean(prix_table$prix))^2)

    sst_1 <- ssr_1 + sse_1
    ssr_1/sst_1
```
Pour ce calcul, on peut utiliser directment le $R^2$, ou calculer les SSR et SST pour trouver $SSR/SST$.
Ainsi, on a que le pourcentage de variance expliquée par cette régression est 90,88%.

### 1.c Analyse du test de student
$H_0$ : Le variable ($m^2$) n'a pas un relation lineére avec le variable réponse(prix)
Pour cette hypothèse, on utilise la valeur $\alpha = 5%$, et si la $p_{value}$ est inferieur a $\alpha$, on rejjete $H_0$.

```{r}
    summary(model1)
```

Dans ce cas, la $p_{value}$ de la variable m2 est de l'ordre de $10^{-14}$, donc ce variable(m2) a une relation lineére avec prix.

## 2. Intervalle de confiance

Pour un niveau de confiance de 95% et un appartement de 90m², on prevoit la valeur $223,915 \pm [209,962;237.8809]$

```{r}
    nouvelle_apart = data.frame(m2 = 90)
    prediction_IC = data.frame(predict(model1, newdata = nouvelle_apart,
                                       interval = 'confidence', level = 0.95))
    
    plot(prix~m2, data = prix_table)
    abline(model1$coefficients, col = 'red')
    points(nouvelle_apart$m2, prediction_IC$fit,col = 'blue', pch=19)
    points(nouvelle_apart$m2, prediction_IC$lwr,col = "blue", pch=19)
    points(nouvelle_apart$m2, prediction_IC$upr,col = "blue", pch=19)
```

## 3. Intervalle de prédiction

Cela donne également la suivante intervalle de prédiction:  $223,915 \pm [154.3086;293.534]$.
En observant cette valeur, on arrive à la conclusion qui *est acceptable de mettre en vente un appartement
de $90m^2$ à 280 Keuros.*

```{r}
    prediction_IP = data.frame(predict(model1, newdata = nouvelle_apart, 
                                       interval = 'prediction', level = 0.95))

    plot(prix~m2, data = prix_table)
    abline(model1$coefficients, col = 'red')

    points(nouvelle_apart$m2, prediction_IP$fit,col = 'blue', pch=19)
    points(nouvelle_apart$m2, prediction_IP$lwr, col = "blue", pch=19)
    points(nouvelle_apart$m2, prediction_IP$upr, col = "blue", pch=19)
```

## 4. Étude des résidus

Les propriétés des estimateurs de la régression linéaire viennent de :
Pour tous les i dans {1,...,n}, $E(\epsilon_{i})=0$ $V(\epsilon_{i}) = \sigma^{2}=cte$
Et pour tous les i different de j, $cov(\epsilon_{i},\epsilon_{j}) = 0$
<!-- $ \forall i \in {1,...,n} , E(\epsilon_{i})=0 et V(\epsilon_{i}) = \sigma^{2}=cte $
- \forall i \ne j, cov(\epsilon_{i},\epsilon_{j}) = 0 -->

Pour ce faire, on étudie le graphe des résidus.

```{r}
par(mfrow=c(2, 2))
plot(model1)
```

On peut vérifier visuellement que ce modèle ne satisfait pas ces hypothèses. D'abord,
on s'attendait une ligne horizontale par le graphe des résidus x la régression
(ça veut dire une moyenne de $\epsilon$ égale à zéro). En outre, on s'attendait le même
par le graphe de la racine des résidus x la régression. Finalement, on observe des points à
l'extérieur de la ligne pointillé. Ce que veut dire qu'il a des points à être enlevés (27). 

```{r}
ecarts = abs(prix_table$prix - model1$fitted.values)
indice = ecarts == max(ecarts)
prix_table[indice,]
c(prix_table$prix[indice], model1$fitted.values[indice])
```

```{r}
filtered_table = prix_table[-c(27),]
model2 = lm(prix~., data = filtered_table)
summary(model2)
par(mfrow=c(2, 2))
plot(model2)
```

On observe que le modèle continue a avoir une mauvaise résultat. Donc on continue à enlever des points.

```{r}
filtered_table_2 = prix_table[-c(27, 26),]
model3 = lm(prix~., data = filtered_table_2)
summary(model3)
par(mfrow=c(2, 2))
plot(model3)

filtered_table_3 = prix_table[-c(27, 26, 24, 21),]
model4 = lm(prix~., data = filtered_table_3)
summary(model4)
par(mfrow=c(2, 2))
plot(model4)
```

Même avec 4 points, on arrive pas a avoir des bonnes résultats. 
Donc on assume que cette stratégie ne va pas marcher.
On vérifie par le graphe des résidus x régression qu'il est possible qu'il ait une rélation 
quadatique. On l'étude. 

```{r}
sqr_table = prix_table
sqr_table['m2'] = sqr_table$m2^2

model_sqr = lm(prix~., data = sqr_table)

summary(model_sqr)
par(mfrow=c(2, 2))
plot(model_sqr)
```

On observe qu'il a des points que a être enlevés (27,16 et 21) avec la rélation quadratique. 
On les enlève.

```{r}
filtered_sqr_table = sqr_table[-c(27, 26, 21),]
model_sqr2 = lm(prix~., data = filtered_sqr_table)
summary(model_sqr2)
par(mfrow=c(2, 2))
plot(model_sqr2)

nouvelle_apart_sqr = data.frame(m2 = 90*90)
prediction_IP = data.frame(predict(model_sqr2, newdata = nouvelle_apart_sqr, 
                                    interval = 'prediction', level = 0.95))
prediction_IP
```

En combinant, on arrive a une meilleur modèle. 
  
On étude après quel effet ce changement aurait sur la valeur de la prédiction faite. 
On arrive aux suivantes valeurs : $204.5488 \pm [176.9511;232.1466]$ (par rapport à $223,915 \pm [154.3086;293.534]$ avant). 
On abserve que l'incertitude a beaucoup diminué, cela qui indique une précision plus importante du modèle actuelle.

<!-- ### 4.b L'incertitude de prédiction -->

# Exercice 02 - La valeur des logements des villes aux alentours de Boston

```{r}
logements_table = read.table(file = "housing.txt", header = TRUE)
head(logements_table)

pairs(logements_table)
jpeg("Plot4.jpeg", width = 30, height = 30, units = 'cm', res = 600)
# plot(logements_table$m2, prix_table$prix)
```

## 1. Quelle est la part de variance expliquée par ce modèle ?

On obtient un modèle avec un $R^{2}$ de 74,06%. C'est-à-dire que
la part de variance expliquée par ce modèle est de 74,06%.

```{r}
    modelb = lm(class~., data = logements_table)
    summary(modelb)
```

## 2. Le modèle de régression est-il significatif dans son ensemble (prendre un risque de première espèce $\alpha$ = 1%) ? Donner l'hypothèse H0, la statistique du test, sa loi sous H0 et la conclusion.

$H_0$ : Les variables n'a pas un relation lineére avec le variable réponse(classe).
Pour cette hypothèse, on utilise la valeur $\alpha = 1%$, et si la $p_{value}$ est inferieur a $\alpha$, on rejjete $H_0$.
On verifie que $p-value < \alpha = 1%$ à partir de la statistique de Fischer, donc on rejjete H0 et on arrive à la conclusion que le
modèle est significatif, pour cette risque de première espèce.

## 3. Quelles sont les variables significatives (prendre un risque de première espèce $\alpha$ = 1%) ? Est-on sûr qu'il n'y en a pas d'autres ?

<!-- En observant le modèle actuelle, on vérifie que les suivantes variables possédent des p-values 
plus importantes que %1 et donc que ne réjettent pas l'hypothèse initiale:

- INDUS: p-value = 73,835%
- AGE: p-value = 95,823%

Les variables significatives donc, sont les 10 autres qui possèdent des p-valeurs moins importantes
que 1%.

- CRIM: p-value = 0.001087 
- ZN: p-value = 0.000779   
- CHAS: p-value = 0.001924
- NOX: p-value = 4.24e-06
- RM: p-value < 2e-16 
- DIS: p-value = 6.01e-13
- RAD: p-value = 5.07e-06
- TAX: p-value = 0.001112
- PTRATIO: p-value = 1.31e-12
- B: p-value = 0.000573
- LSTAT: p-value < 2e-16 -->

Pour identifier les variables significatives, on procède par deux stratégies: AIC et BIC.

```{r}
slm_AIC <- step(modelb, direction="backward", k = 2)
slm_BIC = step(modelb, direction="backward", k = log(nrow(logements_table)))
```

Par les deux méthodes, on arrive au même emsamble de 11 variables significatives 
CRIM, ZN, CHAS, NOX, RM, DIS, RAD, TAX, PTRATIO, B et LSTAT.

C'est-à-dire, que les variables INDUS et AGE ne sont pas significatives et donc ont été
enlevés du modèle

<!-- ## 4. Proposer une méthode pour simplifier le modèle. Expliquer la méthode. La mettre en oeuvre.


## 5. Le modèle obtenu est-il satisfaisant ?



## 6. Proposer un meilleur modèle -->